{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/gdrive/My Drive/EVA_MiLab_Hackathon\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vS_0d48Ojx0q",
        "outputId": "2dc2e9fe-1185-4737-f737-ecf284fe5d41"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/EVA_MiLab_Hackathon\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swig\n",
        "!pip install \"gymnasium[box2d]==1.0.0\""
      ],
      "metadata": {
        "id": "tFsQ0KVWlGZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import count\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from time import gmtime, strftime\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "from agents.cnn_dqn import CNN_DQN_Agent\n",
        "\n",
        "\n",
        "def plot_durations(episode_durations, show_result=False):\n",
        "    plt.figure(1)\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    if show_result:\n",
        "        plt.title('Result')\n",
        "    else:\n",
        "        plt.clf()\n",
        "        plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    # Take 100 episode averages and plot them too\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "    if is_ipython:\n",
        "        if not show_result:\n",
        "            display.display(plt.gcf())\n",
        "            display.clear_output(wait=True)\n",
        "        else:\n",
        "            display.display(plt.gcf())\n",
        "\n",
        "\n",
        "def train_agent(episodes, run_name):\n",
        "    env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\", lap_complete_percent=0.95, domain_randomize=False, continuous=True)\n",
        "\n",
        "    agent = CNN_DQN_Agent(\n",
        "        input_shape=env.observation_space.shape,\n",
        "        action_space=env.action_space,\n",
        "        run_name = RUN_NAME,\n",
        "        **hyperparameters\n",
        "        )\n",
        "    # agent.load_checkpoint()\n",
        "\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        state, info = env.reset()\n",
        "        state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "\n",
        "        for t in count():\n",
        "            action = agent.select_action(state)\n",
        "\n",
        "            observation, reward, terminated, truncated, info = env.step(action.cpu().numpy())\n",
        "            reward = torch.tensor([reward], device=device)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            if terminated:\n",
        "                next_state = None\n",
        "            else:\n",
        "                next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "            agent.memory.push(state.to(agent.device), action.to(agent.device), next_state.to(agent.device), reward.to(agent.device))\n",
        "            agent.train_step()\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "            if done:\n",
        "              agent.episode_durations.append(t + 1)\n",
        "              plot_durations(agent.episode_durations)\n",
        "              break\n",
        "\n",
        "        agent.log_reward(episode, total_reward)\n",
        "\n",
        "        if episode % 50 == 0 and episode > 0:\n",
        "            agent.save_checkpoint(episode)\n",
        "\n",
        "\n",
        "\n",
        "        print(f\"Episode {episode}: Total Reward: {total_reward.cpu().item()}\")\n",
        "\n",
        "    env.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "  is_ipython = 'inline' in matplotlib.get_backend()\n",
        "  if is_ipython:\n",
        "      from IPython import display\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  print(f\"Using device {device}\")\n",
        "\n",
        "  hyperparameters = {\n",
        "      \"batch_size\": 64,\n",
        "      \"gamma\": 0.95,\n",
        "      \"epsilon_start\": 1.0,\n",
        "      \"epsilon_end\": 0.1,\n",
        "      \"tau\": 0.005,\n",
        "      \"epsilon_decay_steps\": 200,\n",
        "      \"learning_rate\": 0.001,\n",
        "      \"replay_buffer_size\": 64,\n",
        "      \"steps_per_target_net_update\": 512\n",
        "  }\n",
        "\n",
        "\n",
        "  RUN_NAME = f\"CNN_DQN_{strftime('%Y%m%d%H%M%S', gmtime())}\"\n",
        "  train_agent(episodes = 300, run_name = RUN_NAME)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "1iP0c4K19dth",
        "outputId": "528c6a45-487d-47c6-a7f3-394d2446d683"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0: Total Reward: -25.795080184936523\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-03ccf63392d3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m   \u001b[0mRUN_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"CNN_DQN_{strftime('%Y%m%d%H%M%S', gmtime())}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m   \u001b[0mtrain_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRUN_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-03ccf63392d3>\u001b[0m in \u001b[0;36mtrain_agent\u001b[0;34m(episodes, run_name)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/MyDrive/EVA_MiLab_Hackathon/agents/cnn_dqn.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m                                                     if s is not None])\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mstate_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0maction_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mreward_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PxvNfRQK1r-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2bNJ8UMwEk4y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}